{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import glove\n",
    "from glove import Corpus\n",
    "\n",
    "import collections\n",
    "import gc \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_notes = pd.read_pickle(\"data/ner_df.p\") # med7\n",
    "w2vec = Word2Vec.load(\"embeddings/word2vec.model\")\n",
    "fasttext = FastText.load(\"embeddings/fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_index_list = []\n",
    "for i in new_notes.itertuples():\n",
    "    \n",
    "    if len(i.ner) == 0:\n",
    "        null_index_list.append(i.Index)\n",
    "new_notes.drop(null_index_list, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med7_ner_data = {}\n",
    "\n",
    "for ii in new_notes.itertuples():\n",
    "    \n",
    "    p_id = ii.SUBJECT_ID\n",
    "    ind = ii.Index\n",
    "    \n",
    "    try:\n",
    "        new_ner = new_notes.loc[ind].ner\n",
    "    except:\n",
    "        new_ner = []\n",
    "            \n",
    "    unique = set()\n",
    "    new_temp = []\n",
    "    \n",
    "    for j in new_ner:\n",
    "        for k in j:\n",
    "            \n",
    "            unique.add(k[0])\n",
    "            new_temp.append(k)\n",
    "\n",
    "    if p_id in med7_ner_data:\n",
    "        for i in new_temp:\n",
    "            med7_ner_data[p_id].append(i)\n",
    "    else:\n",
    "        med7_ner_data[p_id] = new_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(med7_ner_data, \"data/new_ner_word_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(a):\n",
    "    return sum(a) / len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = [med7_ner_data]\n",
    "data_names = [\"new_ner\"]\n",
    "\n",
    "for data, names in zip(data_types, data_names):\n",
    "    new_word2vec = {}\n",
    "    print(\"w2vec starting..\")\n",
    "    for k,v in data.items():\n",
    "\n",
    "        patient_temp = []\n",
    "        for i in v:\n",
    "            try:\n",
    "                patient_temp.append(w2vec[i[0]])\n",
    "            except:\n",
    "                avg = []\n",
    "                num = 0\n",
    "                temp = []\n",
    "\n",
    "                if len(i[0].split(\" \")) > 1:\n",
    "                    for each_word in i[0].split(\" \"):\n",
    "                        try:\n",
    "                            temp = w2vec[each_word]\n",
    "                            avg.append(temp)\n",
    "                            num += 1\n",
    "                        except:\n",
    "                            pass\n",
    "                    if num == 0: continue\n",
    "                    avg = np.asarray(avg)\n",
    "                    t = np.asarray(map(mean, zip(*avg)))\n",
    "                    patient_temp.append(t)\n",
    "        if len(patient_temp) == 0: continue\n",
    "        new_word2vec[k] = patient_temp\n",
    "\n",
    "    #############################################################################\n",
    "    print(\"fasttext starting..\")\n",
    "        \n",
    "    new_fasttextvec = {}\n",
    "\n",
    "    for k,v in data.items():\n",
    "\n",
    "        patient_temp = []\n",
    "\n",
    "        for i in v:\n",
    "            try:\n",
    "                patient_temp.append(fasttext[i[0]])\n",
    "            except:\n",
    "                pass\n",
    "        if len(patient_temp) == 0: continue\n",
    "        new_fasttextvec[k] = patient_temp\n",
    "\n",
    "    #############################################################################    \n",
    "        \n",
    "    print(\"combined starting..\")\n",
    "    new_concatvec = {}\n",
    "\n",
    "    for k,v in data.items():\n",
    "        patient_temp = []\n",
    "    #     if k != 6: continue\n",
    "        for i in v:\n",
    "            w2vec_temp = []\n",
    "            try:\n",
    "                w2vec_temp = w2vec[i[0]]\n",
    "            except:\n",
    "                avg = []\n",
    "                num = 0\n",
    "                temp = []\n",
    "\n",
    "                if len(i[0].split(\" \")) > 1:\n",
    "                    for each_word in i[0].split(\" \"):\n",
    "                        try:\n",
    "                            temp = w2vec[each_word]\n",
    "                            avg.append(temp)\n",
    "                            num += 1\n",
    "                        except:\n",
    "                            pass\n",
    "                    if num == 0: \n",
    "                        w2vec_temp = [0] * 100\n",
    "                    else:\n",
    "                        avg = np.asarray(avg)\n",
    "                        w2vec_temp = np.asarray(map(mean, zip(*avg)))\n",
    "                else:\n",
    "                    w2vec_temp = [0] * 100\n",
    "\n",
    "            fasttemp = fasttext[i[0]]\n",
    "\n",
    "            appended = np.append(fasttemp, w2vec_temp, 0)\n",
    "            patient_temp.append(appended)\n",
    "        if len(patient_temp) == 0: continue\n",
    "        new_concatvec[k] = patient_temp\n",
    "\n",
    "    print(len(new_word2vec), len(new_fasttextvec), len(new_concatvec))\n",
    "    pd.to_pickle(new_word2vec, \"data/\"+names+\"_word2vec_dict.pkl\")\n",
    "    pd.to_pickle(new_fasttextvec, \"data/\"+names+\"_fasttext_dict.pkl\")\n",
    "    pd.to_pickle(new_concatvec, \"data/\"+names+\"_combined_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = set(new_fasttext_dict.keys()).difference(set(new_word2vec_dict))\n",
    "for i in diff:\n",
    "    del new_fasttext_dict[i]\n",
    "    del new_combined_dict[i]\n",
    "print (len(new_word2vec_dict), len(new_fasttext_dict), len(new_combined_dict))\n",
    "\n",
    "\n",
    "pd.to_pickle(new_word2vec_dict, \"data/\"+\"new_ner\"+\"_word2vec_limited_dict.pkl\")\n",
    "pd.to_pickle(new_fasttext_dict, \"data/\"+\"new_ner\"+\"_fasttext_limited_dict.pkl\")\n",
    "pd.to_pickle(new_combined_dict, \"data/\"+\"new_ner\"+\"_combined_limited_dict.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
